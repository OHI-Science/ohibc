---
title: "Spatial stuff"
output: html_document
---
```{r setup, echo = FALSE}
library(dplyr)
library(tidyr)
library(stringr)
```

### Global maps

Find lower-resolution global shapefiles in [ohiprep/globalprep/spatial/downres](https://github.com/OHI-Science/ohiprep/tree/master/globalprep/spatial/downres).  Filename contains info on what regions it covers (all, EEZ, Antarctica, FAO, land), resolution (low and medium), and CRS (lat-long GCS and Mollweide).

Look for med resolution, mol projection (for global equal area), and pull the .shp, .dbf, and .shx files.  The medium resolution files are ~ 11 MB for global coverage, so not huge but also detailed enough for zooming in a bit.

Note that there is no .prj file - so no projection information will come along with these files.  There are two sets of coordinate reference systems:

* mol: Mollweide: equal area projection, units in meters.  
    * EPSG:54009
    * proj.4 string: `'+proj=moll +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs'`
* gcs: lat and long in degrees
    * EPSG:4326 WGS 84
    * proj.4 string: `'+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0'`

### Reading shapefiles

Here are two different ways to read in spatial vector data, each with advantages and disadvantages:
```{r read shapefiles 1}
library(rgdal)
dir_spatial   <- '~/github/ohibc/spatial_data'
layer_bc <- 'ohibc_rgn'

poly_bc_rgn <- readOGR(dsn = path.expand(dir_spatial), layer = layer_bc, stringsAsFactors = FALSE)
  ### note the path.expand(): readOGR can be annoyingly finicky about file names;
  ### it is apparently not a fan of the '~'.  path.expand() gets rid of that.

library(rgeos)
gIsValid(poly_bc_rgn, byid = TRUE) 
  ### checks valid geometries.  We'll deal with the problems later

### you can easily plot polygons, though complex polys take a while
plot(poly_bc_rgn, col = 'light blue', border = 'blue')
```

* `dsn` is the data source name.  It can be a .gdb or a directory with shapefiles in it.  It can be relative or absolute file path.
* `layer` is the layer name.  
    * If you're reading from a .gdb, you can tell it which layer to pull out of that .gdb 
        * `ogrListLayers()` can help identify the layers within the .gdb if you're not sure.
    * If you're reading a .shp (and the associated .dbf etc), then `layer` should be the base name of the shapefile (without the .shp extension).  E.g. if you want `rgn_all_mol_med_res.shp`, then `layer = 'rgn_all_mol_low_res'`.
* `p4s` allows you to input a proj.4 string to indicate a CRS.  If there's already a .prj file associated with this layer, `readOGR()` will automatically read it in.

```{r read shapefiles 2}
library(maptools)
shp_bc <- file.path(dir_spatial, layer_bc)
p4s_bc <- CRS('+proj=aea +lat_1=50 +lat_2=58.5 +lat_0=45 +lon_0=-126 +x_0=1000000 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0')
  ### that's the proj4string for BC Albers projection

poly_bc_rgn <- readShapePoly(fn = shp_bc, proj4string = p4s_bc)

### Not sure why but gIsValid() seems to take much longer on this version of poly_bc_rgn.
# gIsValid(poly_bc_rgn, byid = TRUE)
```

* `fn` is the full filename, leaving off the .shp extension.  This has to be a shapefile (not a .gdb).
* `proj4string` is an optional proj.4 CRS designation.  
    * `readShapePoly()` does not read .prj files, so it will not know the projection unless you manually tell it.  
    * This argument has to be a CRS object, so for example, if you want to set Mollweide you would use: `proj4string = CRS('+proj=moll +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs')`
    
`readOGR()` and `readShapePoly()` also have analogous `write` functions and the like.

`readShapePoly()` seems to be significantly faster for big shapefiles, but doesn't work on .gdb and doesn't automatically read in CRS info, which `readOGR()` can do.  I prefer to stick with `readOGR()` where possible (especially for smaller files) to use projection/CRS information where it is available.

### Understanding Spatial Polygons in R

Once the shapefile is read in, it becomes a SpatialPolygonsDataFrame object (from the `sp` package).  SpatialPolygonsDataFrame store shape info and attributes in different slots, with a structure like this:

* `SpatialPolygonsDataFrame` (top level structure)
    * `data`: a dataframe that holds the attribute table; each row correlates with a polygon in the `polygons` slot.
    * `polygons`: a list of Polygons-class objects, each with their own bits and pieces.  Each item in the list is a complete polygon feature, made up of sub-polygons; and each item in the list corresponds with a row in the dataframe in the `data` slot.
    * `plotOrder`: a vector of integers showing which order to plot the `polygons` list
    * `bbox`: info on the bounding coordinates (x & y min & max).
    * `proj4string`: a CRS object that contains projection or coordinate system info
    
Each of these slots can be accessed by a `@` symbol (rather than a `$` symbol).  If your SpatialPolygonsDataFrame is called `x`, then you can access the attribute table by calling `x@data` and treat it exactly like a regular data frame.

```{r viewing SpatialPolygonsDataFrame slots}
### view the overall structure
summary(poly_bc_rgn)

### check out the attribute table in the data slot
head(poly_bc_rgn@data)

### check out the basics of the polygon features
summary(poly_bc_rgn@polygons)  ### summary of the polygon features

### inspect one of the smaller sub-polys that make up the first polygon
### feature; this one draws a small hole in the main polygon
poly_bc_rgn@polygons[[1]]@Polygons[[2]] 

### check out the CRS info
poly_bc_rgn@proj4string
```

You can also manipulate these slots individually, for example filtering out rows of the dataframe in the `data` slot, but you will also need to treat the other slots accordingly.

### Adding data to polygons

If you'd like to add a column of data to the SpatialPolygonsDataFrame, for example harvest tonnes or a region ID value, you should be able to simply access the `data` slot and treat it like a data frame.
```{r adding data to SpatialPolygonsDataFrame}
### create a data frame of harvest per region.  Note not all regions are represented here!
harv_data <- data.frame(rgn_id   = c(  1,  2,  3,  5,  8),
                        h_tonnes = c(105, 89, 74, 21, 11))

### use left_join to join data to attributes table without changing order
poly_bc_rgn@data <- poly_bc_rgn@data %>%
  left_join(harv_data, by = 'rgn_id')

### note unassigned regions have NA for harvest tonnage
poly_bc_rgn@data
```

### Selecting or filtering polygons based on attributes

Use indexing to select just rows where there is harvest data.  This eliminates the polygons as well as the attribute table rows, and resets the plot order.  Probably safer than filtering on the data slot directly...

```{r selecting/filtering by attribute}
### Select only regions with non-NA harvest values
poly_bc_harvest <- poly_bc_rgn[!is.na(poly_bc_rgn@data$h_tonnes), ]

poly_bc_harvest@data

```

### Fixing projections

Mismatched coordinate reference systems are just as much a pain in the ass in R as they are in ArcGIS or QGIS.  But they're easy to work with once you know how to inspect them and re-project them.

```{r fixing projections}

poly_bc_mpa <- readShapePoly(file.path(dir_spatial, 'mpa_bc'))

plot(poly_bc_rgn, col = rgb(.7, .7, 1), border = rgb(0, 0, 1))
plot(poly_bc_mpa, col = rgb(1, .5, .5, .5), border = rgb(1, 0, 0, .5), add = TRUE)
  ### fun fact: the 'rgb()' function allows you to specify color by 
  ### Red/Green/Blue proportions, but also allows a fourth argument
  ### for 'alpha' (opacity) to create semi-transparent layers

### Note that the MPA polys don't show up on the map!  That's because of a
### CRS error...
poly_bc_mpa@proj4string ### NA!  But it's actually in lat-long coord system.
poly_bc_mpa@proj4string <- CRS('+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0')

### Now that it knows the correct CRS we can reproject to BC Albers:
poly_bc_mpa <- spTransform(poly_bc_mpa, p4s_bc)
poly_bc_mpa@proj4string
poly_bc_rgn@proj4string  ### OK, now they match!

plot(poly_bc_rgn, col = rgb(.7, .7, 1), border = rgb(0, 0, 1))
plot(poly_bc_mpa, col = rgb(1, .5, .5, .3), border = NA, add = TRUE)
  ### The semi-transparency not only looks way awesome, but also
  ### allows us to see whether there are any overlapping polygons!
```

### Rasters and polygons

Extracting raster data into regions defined by a SpatialPolygons* object is pretty easy, using the `extract()` function from the `raster` package.  In next few code chunks we'll determine the extent of marine protected areas within each analysis region in BC's EEZ.

While it is possible to use vector geoprocessing to figure this out, overlapping polygons could be double-counted, so I will instead rasterize the MPA polygons, prioritizing the earliest date of protection.  First step - establish a base raster.

```{r set up base raster}
library(raster)

### use poly_bc_rgn to set raster extents and then round to nearest km
ext <- extent(poly_bc_rgn) 
ext@xmin <- round(ext@xmin - 500, -3); ext@ymin <- round(ext@ymin - 500, -3)
ext@xmax <- round(ext@xmax + 500, -3); ext@ymax <- round(ext@ymax + 500, -3)

reso <- 500 ### BC Albers uses meters as units, set a half-km grid
xcol <- (ext@xmax - ext@xmin)/reso; yrow <- (ext@ymax - ext@ymin)/reso

rast_base <- raster(ext, yrow, xcol, crs = p4s_bc)

rast_base ### inspect it: resolution and extents are nice and clean
```

Next step: rasterize the MPA polygons.

```{r rasterize MPA polygons}
rast_bc_mpa <- rasterize(poly_bc_mpa, rast_base, field = 'STATUS_YR', fun = 'min')
```

Now the big step: raster::extract()

Note that `extract()` is also a function in the `tidyr` package and elsewhere; so I generally force the issue by calling `raster::extract()` just to make sure.

The basic arguments are simply the raster with the interesting data, and the polygon features that define the regions of interest.  There are other arguments, but one set of arguments that may be interesting is `weights` and/or `normalizeWeights`.

* `weights` (logical): if `weights = TRUE`, provides info on how much of a cell was covered by the polygon in question.
* `normalizeWeights`  (logical): if `normalizeWeights = TRUE` then the reported weights will be normalized for each polygon, so the total for each polygon adds up to 1.00.  If you just want the fraction of cell coverage for each cell, set `normalizeWeights = FALSE`.

```{r raster::extract()}
### for this, let's see how much area within 3 nautical miles of the coast
### is designated as a protected area.
poly_bc_3nm <- readShapePoly(fn = file.path(dir_spatial, 'ohibc_offshore_3nm'), proj4string = p4s_bc)

mpa_by_rgn <- raster::extract(rast_bc_mpa, poly_bc_3nm)
# mpa_by_rgn_proportions <- raster::extract(rast_bc_mpa, poly_bc_3nm, weights = TRUE, normalizeWeights = FALSE)
```

`raster::extract()` returns a list of vectors; each list item is a vector of all the cell values contained within one of the polygon features.  At this point, we need to assign a non-generic name to our list items so we can keep track.  The following line names the list items according to the `rgn_id` column in the attribute table contained in `data` (this works since the list is in order of items in the `polygons` slot, which is the same order of attributes in the `data` slot).

```
names(data_list_by_rgn) <- rgn_poly@data$rgn_id
```

To get the data out of annoying list form and into convenient data frame form, this next code chunk does the following:

* create an empty data frame (`data_rgn_df`) that we will build up as we unlist each list item.
* loop over each of the list items (named after the `rgn_id`s in the previous step)
    * create a temporary data frame that stores the `rgn_id` (from loop index) and the entire list of cell values for that particular `rgn_id`
    * tack that temp data frame onto the bottom of the existing `data_rgn_df` (first time through, tacks onto an empty data frame)
    
VoilÃ ! a data frame of cell values by region.

```
data_rgn_df <- data.frame()
for (rgn_id in names(data_list_by_rgn)) {
  temp_df <- data.frame(rgn_id, year = unlist(data_list_by_rgn[[rgn_id]]))
  data_rgn_df <- rbind(data_rgn_df, temp_df)
}
```

To see much of this stuff put into action, check out [OHIBC LSP script](https://github.com/OHI-Science/ohibc/blob/master/lsp/data_prep_lsp.R) and its [associated functions](https://github.com/OHI-Science/ohibc/blob/master/lsp/R/lsp_fxns.R).

For a more complex version that maintains the spatial distribution of the data, check out [OHI global 2015 SPP_ICO goal script](https://github.com/OHI-Science/ohiprep/blob/master/globalprep/SPP_ICO/data_prep_SPP.R) and its [associated functions](https://github.com/OHI-Science/ohiprep/blob/master/globalprep/SPP_ICO/R/spp_fxn.R).

### Alt method of rasterizing polygons

Most of what you need to know about rasters is covered in Jamie's spatial data workshop.  One added point: an alternative method of rasterizing vector data, when `raster::rasterize()` is causing problems.

Note that one major issue with `gdal_rasterize()` (aside from the arcane argument list) is that in the event of overlapping polygons, there's no place for a decision rule as to which polygon value gets assigned to the cell.  If you know there are no overlaps, then this is a very fast option for you.

```
library(gdalUtils)
rast_3nm_gdal <- gdal_rasterize(
    src_datasource = file.path(dir_rgn, 'ohibc_offshore_3nm.shp'),
      # src_datasource needs to be an OGR readable format (e.g. .shp)
      # NOTE: doesn't need source to already be in memory!
    dst_filename = file.path(dir_rast, 'rast_3nm_gdal.tif'), 
      # destination for output
    a = 'rgn_id', 
      # the attribute in the shapefile to be assigned to the cell values
    te = c(ex_xmin, ex_ymin, ex_xmax, ex_ymax), 
      # extents for output raster
    tr = c(500, 500),   
      # resolution for x and y; for my projection, 500 m resolution
    tap = TRUE, 
      # target aligned pixels - align coords of extent of output to values of -tr
    a_nodata = NA, 
      # nodata value for raster; otherwise they will be filled in with zeroes
    output_Raster = TRUE, 
      # return output as a RasterBrick? 
    verbose = TRUE,
  ### unused but interesting arguments:
    # l,     # layer; maybe needed if src_datasource is .gdb, with mult layers
    # of,    # output format; default = GTiff, so I left it stay as default
    # a_srs  # override projection
```

### Plotting polygons

```
get_rgn_df <- function(dsn = str_replace(dir_global, 'ohi-global', 'ohiprep/globalprep/spatial/downres'),
                       layer = NULL, prj = 'gcs') {
  if(is.null(layer)) layer <- sprintf('rgn_eez_%s_low_res', prj)
  rgn_shp <- readOGR(dsn = path.expand(dsn), layer, verbose = FALSE) #, p4s = '+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs')

  ### The ID number for fortify(rgn_shp) is simply the row number within the @data.
  ### So create a lookup table for row number (zero to N-1) to rgn_id.
  rgn_lookup <- data.frame(id     = c(0:(nrow(rgn_shp@data) - 1)), 
                           rgn_id = rgn_shp@data$rgn_id)
  
#   if (prj == 'mol') {
#     cat('transforming spatial polygons data frame to Mollweide projection\n')
#     rgn_shp <- spTransform(rgn_shp, CRS("+proj=moll +R=10567000 +lon_0=0 +x_0=0 +y_0=0 +units=m +towgs84=0,0,0,0,0,0,0 +no_defs"))
#   }
  
  ### Fortify the rgn_eez from shapefile into dataframe.  Then attach the region
  ### ID by the polygon ID (row number from @data)
  rgn_df <- fortify(rgn_shp) %>%
    mutate(id = as.integer(id)) %>%
    left_join(rgn_lookup, by = 'id')
  
  return(rgn_df)
}


get_land_df <- function(dsn = sprintf('%s/../ohiprep/globalprep/spatial/downres', dir_global),
                        layer = 'rgn_land_mol_low_res') {
### gets Mollweide land forms for plotting.
  rgn_shp <- readOGR(dsn = path.expand(dsn), layer, verbose = FALSE)
  
  ### Fortify the rgn_eez from shapefile into dataframe.  Then attach the region
  ### ID by the polygon ID (row number from @data)
  rgn_df <- fortify(rgn_shp)
  
  return(rgn_df)
}

get_ocean_df <- function(dsn = sprintf('%s/../ohiprep/globalprep/spatial/downres', dir_global),
                         layer = 'rgn_all_mol_low_res') {
  ### gets Mollweide ocean regions (all) for plotting.
  rgn_shp <- readOGR(dsn = path.expand(dsn), layer, verbose = FALSE)
  
  ### Fortify the rgn_eez from shapefile into dataframe.  Then attach the region
  ### ID by the polygon ID (row number from @data)
  rgn_df <- fortify(rgn_shp)
  
  return(rgn_df)
}

plot_scores <- function(rgn_df, fld, fig_save = NULL, prj = 'gcs',
                        title = NULL, leg_title = FALSE, leg_on = TRUE) {
  if(is.null(title)) {
    fld_name <- expand_fld(fld)
    title <- sprintf('Scores: %s (%s)', fld, fld_name)
  }
  
  col.brks  <- seq(0, 100, length.out = 6)

  df_plot <- ggplot(data = rgn_df, aes(x = long, y = lat, group = group, fill = val)) +  
    theme(axis.ticks = element_blank(), axis.text = element_blank(),
          text = element_text(family = 'Helvetica', color = 'gray30', size = 12),
          plot.title = element_text(size = rel(1.5), hjust = 0, face = 'bold'),
          legend.position = ifelse(leg_on, 'right', 'none')) + 
    scale_fill_gradientn(colours = brewer.pal(10, 'RdYlBu'), space = 'Lab', na.value = 'gray80',
                         breaks = col.brks, labels = col.brks, limits = c(0, 100)) + 
    labs(title = title, 
         fill  = ifelse(leg_title & leg_on, fld, ''),
         x = NULL, y = NULL) 
  if(prj == 'mol'){
  ### For Mollweide, 'border()' doesn't seem to work.  Load in land and ocean polygons to plot directly.
    if(!exists('land_poly'))
      land_poly <- get_land_df()
    if(!exists('ocean_poly'))
      ocean_poly <- get_ocean_df()
    
    df_plot <- df_plot +
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank()) +
      geom_polygon(data = ocean_poly, color = 'gray92', fill = 'gray92', size = 0.25) +
      geom_polygon(color = 'gray80', size = 0.1) +
      geom_polygon(data = land_poly, color = 'gray40', fill = 'gray45', size = 0.25)
    ### df_plot order: oceans (light grey), then EEZ score polygons, then land polygons (dark grey).
  } else {
  ### For default projection, use standard grey background for oceans, and use borders() for land forms.
    df_plot <- df_plot +   
      geom_polygon(color = 'gray80', size = 0.1) +
      borders('world', color = 'gray40', fill = 'gray45', size = .25) + # create a layer of borders
      scale_x_continuous(breaks = seq(-180, 180, by = 30), expand = c(0, 2)) +
      scale_y_continuous(breaks = seq( -90,  90, by = 30), expand = c(0, 2))
  }

  if(!is.null(fig_save)) {
    cat(sprintf('Saving map to %s...\n', fig_save))
    ggsave(fig_save, width = 10, height = 6)
    return(invisible(df_plot))
  } else {
    return(df_plot)
  }  
}
```